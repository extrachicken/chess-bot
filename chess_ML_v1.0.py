# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1juQVq31xxd8A-TPTpQJKkcsZZPLHuVe8
"""

# === Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ ===
import os
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# === ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ===
BATCH_SIZE = 64
EPOCHS_PER_CHUNK = 1
FILES_PER_CHUNK = 10
PATIENCE = 5
resume_training = False

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
checkpoint_path = "checkpoint.pt"
best_model_path = "best_model.pt"
all_files = sorted(glob.glob("/content/batches/chess_batch_*.npz"))

import json

label_map_path = "label_map.json"

# === Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° label2new ===
if os.path.exists(label_map_path):
    with open(label_map_path, "r") as f:
        label2new = json.load(f)
        label2new = {int(k): int(v) for k, v in label2new.items()}
    print(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ label2new Ğ¸Ğ· {label_map_path}")
else:
    all_labels = []
    for file in all_files:
        with np.load(file) as d:
            all_labels.extend(d['y'].tolist())
    unique_labels = sorted(set(all_labels))
    label2new = {old: new for new, old in enumerate(unique_labels)}
    with open(label_map_path, "w") as f:
        json.dump(label2new, f)
    print(f"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ label2new Ğ² {label_map_path}")

num_classes = len(label2new)

# === Ğ’ÑĞµ .npz Ñ„Ğ°Ğ¹Ğ»Ñ‹ ===
num_chunks = (len(all_files) + FILES_PER_CHUNK - 1) // FILES_PER_CHUNK

# === ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¼ĞµÑ‚Ğ¾Ğº ===
all_labels = []
for file in all_files:
    with np.load(file) as d:
        all_labels.extend(d['y'].tolist())

unique_labels = sorted(set(all_labels))
label2new = {old: new for new, old in enumerate(unique_labels)}
num_classes = len(label2new)

class ChessNPZDataset(Dataset):
    def __init__(self, file_list, label2new):
        self.file_list = file_list
        self.label2new = label2new
        self.data_index = []
        self.cache = {}

        for f in self.file_list:
            with np.load(f) as d:
                length = d['x'].shape[0]
                self.data_index.extend([(f, i) for i in range(length)])

    def __getitem__(self, idx):
        file_path, local_idx = self.data_index[idx]
        if file_path not in self.cache:
            self.cache[file_path] = np.load(file_path)
        data = self.cache[file_path]

        x = torch.tensor(data['x'][local_idx], dtype=torch.float32)
        y_raw = data['y'][local_idx]
        y = torch.tensor(self.label2new[y_raw], dtype=torch.long)
        return x, y

    def __len__(self):
        return len(self.data_index)

class ChessPolicyNet(nn.Module):
    def __init__(self, in_channels=110, num_classes=4672):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.fc = nn.Linear(256 * 8 * 8, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

import time
model = ChessPolicyNet(in_channels=110, num_classes=num_classes).to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

start_chunk = 0
BEST_ACC = 0.0
patience_counter = 0

# === Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ===
if resume_training and os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_chunk = checkpoint['chunk'] + 1
    BEST_ACC = checkpoint['best_acc']
    patience_counter = checkpoint.get('patience_counter', 0)
    print(f"ğŸ” ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ñ Ñ‡Ğ°Ğ½ĞºĞ° {start_chunk}, BEST_ACC={BEST_ACC:.4f}")


for chunk_idx in range(start_chunk, num_chunks):
    print(f"\nğŸ“¦ Ğ§Ğ°Ğ½Ğº {chunk_idx+1}/{num_chunks}")

    # === ĞŸĞ¾Ğ´Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ===
    chunk_files = all_files[chunk_idx * FILES_PER_CHUNK : (chunk_idx + 1) * FILES_PER_CHUNK]
    dataset = ChessNPZDataset(chunk_files, label2new)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

    # === ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞµ ===
    for epoch in range(EPOCHS_PER_CHUNK):
        start = time.time()
        model.train()
        total_loss = 0
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"ğŸ•’ Ğ­Ğ¿Ğ¾Ñ…Ğ° {epoch+1} Ğ·Ğ°Ğ½ÑĞ»Ğ°: {time.time() - start:.2f} ÑĞµĞº")
        print(f"Loss: {total_loss:.2f}")

    # === ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞµ ===
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)
    acc = correct / total
    print(f"  âœ… Accuracy on chunk: {acc:.4f}")

    # === Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ===
    torch.save({
        'chunk': chunk_idx,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_acc': BEST_ACC,
        'patience_counter': patience_counter,
    }, checkpoint_path)

    # === Early Stopping ===
    if acc > BEST_ACC:
        BEST_ACC = acc
        patience_counter = 0
        torch.save(model.state_dict(), best_model_path)
        print("  ğŸ”¥ New best model saved!")
    else:
        patience_counter += 1
        print(f"  â³ No improvement. Patience {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print("  ğŸ›‘ Early stopping triggered.")
            break

print("\nğŸ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾.")

print("ğŸ’» Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾:", DEVICE)