# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1juQVq31xxd8A-TPTpQJKkcsZZPLHuVe8
"""

# === Импорт ===
import os
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# === Настройки ===
BATCH_SIZE = 64
EPOCHS_PER_CHUNK = 1
FILES_PER_CHUNK = 10
PATIENCE = 5
resume_training = False

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
checkpoint_path = "checkpoint.pt"
best_model_path = "best_model.pt"
all_files = sorted(glob.glob("/content/batches/chess_batch_*.npz"))

import json

label_map_path = "label_map.json"

# === Генерация или загрузка label2new ===
if os.path.exists(label_map_path):
    with open(label_map_path, "r") as f:
        label2new = json.load(f)
        label2new = {int(k): int(v) for k, v in label2new.items()}
    print(f"✅ Загружен label2new из {label_map_path}")
else:
    all_labels = []
    for file in all_files:
        with np.load(file) as d:
            all_labels.extend(d['y'].tolist())
    unique_labels = sorted(set(all_labels))
    label2new = {old: new for new, old in enumerate(unique_labels)}
    with open(label_map_path, "w") as f:
        json.dump(label2new, f)
    print(f"💾 Сохранён label2new в {label_map_path}")

num_classes = len(label2new)

# === Все .npz файлы ===
num_chunks = (len(all_files) + FILES_PER_CHUNK - 1) // FILES_PER_CHUNK

# === Построение словаря меток ===
all_labels = []
for file in all_files:
    with np.load(file) as d:
        all_labels.extend(d['y'].tolist())

unique_labels = sorted(set(all_labels))
label2new = {old: new for new, old in enumerate(unique_labels)}
num_classes = len(label2new)

class ChessNPZDataset(Dataset):
    def __init__(self, file_list, label2new):
        self.file_list = file_list
        self.label2new = label2new
        self.data_index = []
        self.cache = {}

        for f in self.file_list:
            with np.load(f) as d:
                length = d['x'].shape[0]
                self.data_index.extend([(f, i) for i in range(length)])

    def __getitem__(self, idx):
        file_path, local_idx = self.data_index[idx]
        if file_path not in self.cache:
            self.cache[file_path] = np.load(file_path)
        data = self.cache[file_path]

        x = torch.tensor(data['x'][local_idx], dtype=torch.float32)
        y_raw = data['y'][local_idx]
        y = torch.tensor(self.label2new[y_raw], dtype=torch.long)
        return x, y

    def __len__(self):
        return len(self.data_index)

class ChessPolicyNet(nn.Module):
    def __init__(self, in_channels=110, num_classes=4672):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.fc = nn.Linear(256 * 8 * 8, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

import time
model = ChessPolicyNet(in_channels=110, num_classes=num_classes).to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

start_chunk = 0
BEST_ACC = 0.0
patience_counter = 0

# === Дообучение ===
if resume_training and os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_chunk = checkpoint['chunk'] + 1
    BEST_ACC = checkpoint['best_acc']
    patience_counter = checkpoint.get('patience_counter', 0)
    print(f"🔁 Продолжаем с чанка {start_chunk}, BEST_ACC={BEST_ACC:.4f}")


for chunk_idx in range(start_chunk, num_chunks):
    print(f"\n📦 Чанк {chunk_idx+1}/{num_chunks}")

    # === Подгружаем часть файлов ===
    chunk_files = all_files[chunk_idx * FILES_PER_CHUNK : (chunk_idx + 1) * FILES_PER_CHUNK]
    dataset = ChessNPZDataset(chunk_files, label2new)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

    # === Обучение на чанке ===
    for epoch in range(EPOCHS_PER_CHUNK):
        start = time.time()
        model.train()
        total_loss = 0
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"🕒 Эпоха {epoch+1} заняла: {time.time() - start:.2f} сек")
        print(f"Loss: {total_loss:.2f}")

    # === Оценка на чанке ===
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)
    acc = correct / total
    print(f"  ✅ Accuracy on chunk: {acc:.4f}")

    # === Сохраняем состояние ===
    torch.save({
        'chunk': chunk_idx,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_acc': BEST_ACC,
        'patience_counter': patience_counter,
    }, checkpoint_path)

    # === Early Stopping ===
    if acc > BEST_ACC:
        BEST_ACC = acc
        patience_counter = 0
        torch.save(model.state_dict(), best_model_path)
        print("  🔥 New best model saved!")
    else:
        patience_counter += 1
        print(f"  ⏳ No improvement. Patience {patience_counter}/{PATIENCE}")
        if patience_counter >= PATIENCE:
            print("  🛑 Early stopping triggered.")
            break

print("\n🏁 Обучение завершено.")

print("💻 Устройство:", DEVICE)